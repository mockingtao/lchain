{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from apikey import apikey, langchain_apikey\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = apikey\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_apikey\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.记录Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1.直接通过LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No extra code is needed to log a trace to LangSmith when using LangChain Python.\n",
    "# Just run your LangChain code as you normally would with the LANGCHAIN_TRACING_V2 environment variable set to 'true' and the LANGCHAIN_API_KEY environment variable set to your API key.\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 使用 LangChain Python 时，无需额外的代码即可记录对 LangSmith 的跟踪。\n",
    "# 只需像往常一样运行您的 LangChain 代码，将 LANGCHAIN_TRACING_V2 环境变量设置为“true”，并将 LANGCHAIN_API_KEY 环境变量设置为您的 API 密钥。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You are a helpful assistant. Please respond to the user's request only based on the given context.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Question: {question}\\nContext: {context}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10c2f0590>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10c231810>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n",
    "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"During this morning's meeting, all world conflict was resolved.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can you summarize this morning's meetings?\"\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2.使用 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the example below, ensure the environment variable OPENAI_API_KEY is set\n",
    "from typing import Any, Iterable\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.run_trees import RunTree\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTION 1: Use RunTree API (more explicit) ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be a user input to your app\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "# Create a top-level run\n",
    "pipeline = RunTree(\n",
    "    name=\"Chat Pipeline Run Tree\",\n",
    "    run_type=\"chain\",\n",
    "    inputs={\"question\": question}\n",
    ")\n",
    "\n",
    "# This can be retrieved in a retrieval step\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "]\n",
    "\n",
    "# Create a child run\n",
    "child_llm_run = pipeline.create_child(\n",
    "    name=\"OpenAI Call\",\n",
    "    run_type=\"llm\",\n",
    "    inputs={\"messages\": messages},\n",
    ")\n",
    "\n",
    "# Generate a completion\n",
    "client = openai.Client()\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=messages\n",
    ")\n",
    "\n",
    "# End the runs and log them\n",
    "child_llm_run.end(outputs=chat_completion)\n",
    "child_llm_run.post()\n",
    "\n",
    "pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})\n",
    "pipeline.post()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTION 2: Use traceable decorator and OpenAI Client ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: wrap the openai client to add tracing directly\n",
    "# This can also be done with a traceable decorator\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable(run_type=\"tool\", name=\"Retrieve Context\")\n",
    "def my_tool(question: str) -> str:\n",
    "    return \"During this morning's meeting, we solved all world conflict.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"During this morning's meeting, world conflicts were resolved.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@traceable(name=\"Chat Pipeline Traceable\")\n",
    "def chat_pipeline(question: str):\n",
    "    context = my_tool(question)\n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "    ]\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=messages\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "chat_pipeline(\"Can you summarize this morning's meetings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc：这里的核心是通过 traceable 装饰器，整体来看相比于 RunTree 要更加方便一些；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 使用python API方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send your API Key in the request headers\n",
    "# headers = {\"x-api-key\": \"<YOUR API KEY>\"}\n",
    "headers = {\"x-api-key\": langchain_apikey}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_run(run_id, name, run_type, inputs, parent_id=None):\n",
    "    \"\"\"Function to post a new run to the API.\"\"\"\n",
    "    data = {\n",
    "        \"id\": run_id.hex,\n",
    "        \"name\": name,\n",
    "        \"run_type\": run_type,\n",
    "        \"inputs\": inputs,\n",
    "        \"start_time\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "    if parent_id:\n",
    "        data[\"parent_run_id\"] = parent_id.hex\n",
    "    requests.post(\n",
    "        \"https://api.smith.langchain.com/runs\",\n",
    "        json=data,\n",
    "        headers=headers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_run(run_id, outputs):\n",
    "    \"\"\"Function to patch a run with outputs.\"\"\"\n",
    "    requests.patch(\n",
    "        f\"https://api.smith.langchain.com/runs/{run_id}\",\n",
    "        json={\n",
    "            \"outputs\": outputs,\n",
    "            \"end_time\": datetime.utcnow().isoformat(),\n",
    "        },\n",
    "        headers=headers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This can be a user input to your app\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "\n",
    "# This can be retrieved in a retrieval step\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/0511lkxx3c3cmf9jbmg0q27r0000gn/T/ipykernel_65306/33855674.py:14: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  patch_run(child_run_id, chat_completion.dict())\n"
     ]
    }
   ],
   "source": [
    "# Create parent run\n",
    "parent_run_id = uuid4()\n",
    "post_run(parent_run_id, \"Chat Pipeline\", \"chain\", {\"question\": question})\n",
    "\n",
    "# Create child run\n",
    "child_run_id = uuid4()\n",
    "post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n",
    "\n",
    "# Generate a completion\n",
    "client = openai.Client()\n",
    "chat_completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "# End runs\n",
    "patch_run(child_run_id, chat_completion.dict())\n",
    "patch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.通过 web链接来看Traces\n",
    "\n",
    "链接：https://smith.langchain.com/o/1040bdf6-7131-5c03-b7be-fb1b4779027f/projects/p/5da73c3f-abb4-4b8e-8af4-51321f6b206a?timeModel=%7B%22duration%22%3A%227d%22%7D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.设置跟踪的采样率:\n",
    "\n",
    "要对记录到 LangSmith 的跟踪数进行下采样，请将 LANGCHAIN_TRACING_SAMPLING_RATE 环境变量设置为介于 0（无跟踪）和 1（所有跟踪）之间的任何浮点数。\n",
    "\n",
    "这需要 python SDK 版本 >= 0.0.84 和 JS SDK 版本 >= 0.0.64。例如，设置以下环境变量将过滤掉 25% 的跟踪：\n",
    "\n",
    "export LANGCHAIN_TRACING_SAMPLING_RATE=0.75\n",
    "\n",
    "这适用于可跟踪的装饰器和 RunTree 对象。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.关闭 Tracing\n",
    "\n",
    "如果您决定不再要跟踪运行，则可以删除配置为首先开始跟踪的环境变量。通过取消设置LANGCHAIN_TRACING_V2环境变量，跟踪将不再记录到 LangSmith。请注意，这目前不会影响 RunTree 对象。\n",
    "\n",
    "This setting works both with LangChain and the LangSmith SDK, in both Python and TypeScript.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.获取已记录运行的Run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from uuid import uuid4\n",
    "from langsmith import traceable\n",
    "from langsmith.run_trees import RunTree\n",
    "from langsmith.wrappers import wrap_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "    { \"role\": \"user\", \"content\": \"Is sunshine good for you?\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunTree(id=UUID('02ec5c39-45c6-4aad-8fbb-8fbc1c97c9a3'), name='OpenAI Call RunTree', start_time=datetime.datetime(2024, 3, 18, 11, 36, 46, 200445, tzinfo=datetime.timezone.utc), run_type='llm', end_time=None, extra={}, error=None, serialized={'name': 'OpenAI Call RunTree'}, events=None, inputs={'messages': [{'role': 'system', 'content': \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"}, {'role': 'user', 'content': 'Is sunshine good for you?'}]}, outputs=None, reference_example_id=None, parent_run_id=None, tags=None, parent_run=None, child_runs=[], session_name='default', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240318T113646200445Z02ec5c39-45c6-4aad-8fbb-8fbc1c97c9a3', trace_id=UUID('02ec5c39-45c6-4aad-8fbb-8fbc1c97c9a3'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect run ID using RunTree\n",
    "run_id = uuid4()\n",
    "rt = RunTree(\n",
    "    name=\"OpenAI Call RunTree\",\n",
    "    run_type=\"llm\",\n",
    "    inputs={\"messages\": messages},\n",
    "    id=run_id\n",
    ")\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunTree Run ID:  02ec5c39-45c6-4aad-8fbb-8fbc1c97c9a3\n"
     ]
    }
   ],
   "source": [
    "client = openai.Client()\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=messages\n",
    ")\n",
    "rt.end(outputs=chat_completion)\n",
    "rt.post()\n",
    "print(\"RunTree Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Wrapper Run ID:  fbeb7d8b-d151-41d1-a177-9d322b556a04\n"
     ]
    }
   ],
   "source": [
    "# Collect run ID using openai_wrapper\n",
    "run_id = uuid4()\n",
    "client = wrap_openai(openai.Client())\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=messages, langsmith_extra={\n",
    "        \"run_id\": run_id,\n",
    "    },\n",
    ")\n",
    "print(\"OpenAI Wrapper Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceable Run ID:  1d93c6e2-c333-4347-820d-af845938c006\n"
     ]
    }
   ],
   "source": [
    "# Collect run id using traceable decorator\n",
    "run_id = uuid4()\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    name=\"OpenAI Call Decorator\",\n",
    ")\n",
    "def call_openai(\n",
    "    messages: list[dict], model: str = \"gpt-3.5-turbo\"\n",
    ") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    ).choices[0].message.content\n",
    "result = call_openai(\n",
    "    messages,\n",
    "    langsmith_extra={\n",
    "        \"run_id\": run_id,\n",
    "    },\n",
    ")\n",
    "print(\"Traceable Run ID: \", run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.获取记录的 Run URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行将记录到您配置的任何项目（如果未设置，则为“默认”），您可以通过打开相应的项目详细信息页面来查看它们。\n",
    "\n",
    "要以编程方式访问运行的 URL，您可以使用 LangSmith 客户端。\n",
    "\n",
    "下面是一个示例。若要获取运行的运行 ID，可以按照此处的指南进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://smith.langchain.com/o/1040bdf6-7131-5c03-b7be-fb1b4779027f/projects/p/5da73c3f-abb4-4b8e-8af4-51321f6b206a/r/1d93c6e2-c333-4347-820d-af845938c006?trace_id=1d93c6e2-c333-4347-820d-af845938c006&start_time=2024-03-18T11:38:48.849409\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "run = client.read_run(\"1d93c6e2-c333-4347-820d-af845938c006\")\n",
    "print(run.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.删除项目中的traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "client.delete_project(project_name=\"<project_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
