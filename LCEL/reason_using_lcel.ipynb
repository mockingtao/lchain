{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么使用 LCEL 的原因：\n",
    "\n",
    "\n",
    "1. 统一接口:每个LCEL对象实现Runnable接口，该接口定义了一组通用的调用方法(invoke、batch、stream、ainvoke等)。这使得LCEL对象链也可以自动支持这些调用。也就是说，每个LCEL对象链本身都是一个LCEL对象。\n",
    "\n",
    "2. 组合原语:LCEL提供了许多原语，可以轻松地组合链、并行化组件、添加回退、动态配置内部链等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from apikey import apikey, langchain_apikey\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = apikey\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_apikey\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "prompt_template = \"\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"ice cream\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with LCEL\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")\n",
    "\n",
    "# cc：对于chain 中第一个 {\"topic\": RunnablePassthrough} 要怎么理解？\n",
    "# RunnablePassthrough的 用法是什么？官方文档：https://python.langchain.com/docs/expression_language/how_to/passthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  topic: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell me a short joke about {topic}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11b9dd610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11bac2d50>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "\n",
    "from typing import Iterator\n",
    "\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content\n",
    "\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    return stream_chat_model([{\"role\":\"user\", \"content\":prompt_value}])\n",
    "\n",
    "for chunk in stream_chain(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream truck break down? It couldn't handle the rocky road!"
     ]
    }
   ],
   "source": [
    "# with LCEL\n",
    "\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# cc：有了chain 就有了”一切“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def batch_chain(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(invoke_chain, topics))\n",
    "    \n",
    "batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LECL\n",
    "\n",
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
    "\n",
    "# cc：有个chain 就有了”一切“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "\n",
    "async_client = openai.AsyncOpenAI()\n",
    "\n",
    "async def acall_chat_model(messages: List[dict]) -> str:\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def ainvoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\":prompt_value}]\n",
    "    return await acall_chat_model(messages)\n",
    "\n",
    "await ainvoke_chain(\"ice cream\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LCEL\n",
    "\n",
    "await chain.ainvoke(\"ice cream\")\n",
    "\n",
    "# cc：有了chain 就有了”一切“\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "\n",
    "import openai\n",
    "import asyncio\n",
    "\n",
    "async def abatch_chain(topics: list) -> list:\n",
    "    coros = map(ainvoke_chain, topics)\n",
    "    return await asyncio.lgather(*coros)\n",
    "\n",
    "await abatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LCEL\n",
    "await chain.abatch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM instead of chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "\n",
    "def call_llm(prompt_value: str) -> str:\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt_value,\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "def invoke_llm_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    return call_llm(prompt_value)\n",
    "\n",
    "invoke_llm_chain[\"ice cream\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "llm_chain.invoke(\"ice cream\")\n",
    "\n",
    "# cc：chain 中“组成”的替换也很方便；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different model provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "import anthropic\n",
    "\n",
    "anthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "def call_anthropic(prompt_value: str) -> str:\n",
    "    response = anthropic_client.completion.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_token_to_sample=256,\n",
    "    )\n",
    "    return response.completion\n",
    "\n",
    "def invoke_anthropic_chain(topic: str) -> str:\n",
    "    prompt_value = anthropic_template.format(topic=topic)\n",
    "    return call_anthropic(prompt_value)\n",
    "\n",
    "invoke_anthropic_chain(\"ice cream\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LCEL\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "\n",
    "anthropic_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | anthropic\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime configurability\n",
    "\n",
    "如果我们想让聊天模型或 LLM 在运行时可配置:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "def invoke_configurable_chain(topic: str, *, model: str = \"chat_openai\") -> str:\n",
    "    if model == \"chat_openai\":\n",
    "        return invoke_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        return invoke_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        return invoke_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "    \n",
    "def stream_configurable_chain(topic: str, *, model: str = \"chat_openai\") -> Iterator[str]:\n",
    "    if model == \"chat_openai\":\n",
    "        return stream_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        # Note we haven't implemented this yet.\n",
    "        return stream_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        # Note we haven't implemented this yet\n",
    "        return stream_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "def batch_configurable_chain(topics: List[str], *, model: str = \"chat_openai\") -> List[str]:\n",
    "    # You get the idea\n",
    "    ...\n",
    "\n",
    "\n",
    "async def abatch_configurable_chain(topics: List[str], *, model: str = \"chat_openai\") -> List[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "invoke_configurable_chain(\"ice cream\", model=\"openai\")\n",
    "stream = stream_configurable_chain(\n",
    "    \"ice_cream\", \n",
    "    model=\"anthropic\"\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LCEL\n",
    "\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"chat_openai\",\n",
    "    openai=llm,\n",
    "    anthropic=anthropic,\n",
    ")\n",
    "\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | configurable_model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "configurable_chain.invoke(\n",
    "    \"ice cream\",\n",
    "    config={\"model\": \"openai\"}\n",
    ")\n",
    "\n",
    "stream = configurable_chain.stream(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"anthropic\"},\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withour LCEL\n",
    "\n",
    "def invoke_anthropic_chain_with_logging(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = anthropic_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    output = call_anthropic(prompt_value)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "invoke_anthropic_chain_with_logging(\"ice cream\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LCEL\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "anthropic_chain.invoke(\"ice cream\")\n",
    "\n",
    "# cc：langsmith 真的好用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallbacks\n",
    "\n",
    "如果我们想添加回退逻辑，以防一个模型 API 出现故障\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LCEL\n",
    "def invoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return invoke_chain(topic)\n",
    "    except Exception:\n",
    "        return invoke_anthropic_chain(topic)\n",
    "\n",
    "async def ainvoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return await ainvoke_chain(topic)\n",
    "    except Exception:\n",
    "        # Note: we haven't actually implemented this.\n",
    "        return await ainvoke_anthropic_chain(topic)\n",
    "\n",
    "async def batch_chain_with_fallback(topics: List[str]) -> str:\n",
    "    try:\n",
    "        return batch_chain(topics)\n",
    "    except Exception:\n",
    "        # Note: we haven't actually implemented this.\n",
    "        return batch_anthropic_chain(topics)\n",
    "\n",
    "invoke_chain_with_fallback(\"ice cream\")\n",
    "# await ainvoke_chain_with_fallback(\"ice cream\")\n",
    "batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ICEL\n",
    "fallback_chain = chain.with_fallbacks([anthropic_chain])\n",
    "\n",
    "\n",
    "fallback_chain.invoke(\"ice cream\")\n",
    "# await fallback_chain.ainvoke(\"ice cream\")\n",
    "fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code comparison\n",
    "\n",
    "这对比真的是很明显了~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without ICEL\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import anthropic\n",
    "import openai\n",
    "\n",
    "# 模板\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "anthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\n",
    "# llm\n",
    "client = openai.OpenAI()\n",
    "async_client = openai.AsyncOpenAI()\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "# openai 的chat 通用方法\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 得到openai chat 的回复\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    output = call_chat_model(messages)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "# openai的流式回复 chat 通用方法\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content\n",
    "\n",
    "# 得到openai chat 的流式回复\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
    "    for chunk in stream:\n",
    "        print(f\"Token: {chunk}\", end=\"\")\n",
    "        yield chunk\n",
    "\n",
    "# openai请求的并行处理\n",
    "def batch_chain(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(invoke_chain, topics))\n",
    "\n",
    "# openai 模型替换（非chat接口）\n",
    "def call_llm(prompt_value: str) -> str:\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt_value,\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "# 得到openai llm 的回复\n",
    "def invoke_llm_chain(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    output = call_llm(prompt_value)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "# claude 模型调用\n",
    "def call_anthropic(prompt_value: str) -> str:\n",
    "    response = anthropic_client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    return response.completion   \n",
    "\n",
    "# 得到 claude 返回\n",
    "def invoke_anthropic_chain(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = anthropic_template.format(topic=topic)\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    output = call_anthropic(prompt_value)\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "# 一些异步/并行请求，同openai\n",
    "async def ainvoke_anthropic_chain(topic: str) -> str:\n",
    "    ...\n",
    "\n",
    "def stream_anthropic_chain(topic: str) -> Iterator[str]:\n",
    "    ...\n",
    "\n",
    "def batch_anthropic_chain(topics: List[str]) -> List[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "# 配置化不同模型返回\n",
    "def invoke_configurable_chain(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> str:\n",
    "    if model == \"chat_openai\":\n",
    "        return invoke_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        return invoke_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        return invoke_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "# 配置不同模型的流式返回\n",
    "def stream_configurable_chain(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> Iterator[str]:\n",
    "    if model == \"chat_openai\":\n",
    "        return stream_chain(topic)\n",
    "    elif model == \"openai\":\n",
    "        # Note we haven't implemented this yet.\n",
    "        return stream_llm_chain(topic)\n",
    "    elif model == \"anthropic\":\n",
    "        # Note we haven't implemented this yet\n",
    "        return stream_anthropic_chain(topic)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Received invalid model '{model}'.\"\n",
    "            \" Expected one of chat_openai, openai, anthropic\"\n",
    "        )\n",
    "\n",
    "def batch_configurable_chain(\n",
    "    topics: List[str], \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "async def abatch_configurable_chain(\n",
    "    topics: List[str], \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "# 回退逻辑\n",
    "def invoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return invoke_chain(topic)\n",
    "    except Exception:\n",
    "        return invoke_anthropic_chain(topic)\n",
    "\n",
    "async def ainvoke_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return await ainvoke_chain(topic)\n",
    "    except Exception:\n",
    "        return await ainvoke_anthropic_chain(topic)\n",
    "\n",
    "async def batch_chain_with_fallback(topics: List[str]) -> str:\n",
    "    try:\n",
    "        return batch_chain(topics)\n",
    "    except Exception:\n",
    "        return batch_anthropic_chain(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with LCEL\n",
    "\n",
    "import os\n",
    "\n",
    "# llm\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# langchain 核心模块\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 环境变量配置，主要for langsmith 进行日志跟踪\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = (\n",
    "    chat_openai.with_fallbacks([anthropic]).configurable_alternatives(\n",
    "        ConfigurableField(id=\"model\"),\n",
    "        default_key=\"chat_openai\",\n",
    "        openai=openai,\n",
    "        anthropic=anthropic,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
