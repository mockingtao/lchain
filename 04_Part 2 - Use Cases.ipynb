{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {},
   "source": [
    "# LangChain Cookbook Part 2: Use CasesğŸ‘¨â€ğŸ³ğŸ‘©â€ğŸ³"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "*This cookbook is based off the [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "1. Inspire you to build\n",
    "2. Provide an introductory understanding of the main use cases of LangChain via [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) examples and code snippets. For an introduction to the *fundamentals* of LangChain check out [Cookbook Part 1: Fundamentals](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb).\n",
    "\n",
    "**LangChain Links:**\n",
    "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
    "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **What is LangChain?**\n",
    "> LangChain is a framework for developing applications powered by language models.\n",
    "*[Source](https://blog.langchain.dev/announcing-our-10m-seed-round-led-by-benchmark/#:~:text=LangChain%20is%20a%20framework%20for%20developing%20applications%20powered%20by%20language%20models)*\n",
    "\n",
    "**TLDR**: LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
    "\n",
    "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\n",
    "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next\n",
    "\n",
    "### **Why LangChain?**\n",
    "1. **Components** - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. **Customized Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. **Speed ğŸš¢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. **Community ğŸ‘¥** - Wonderful [discord](https://discord.gg/6adMQxSpJS) and community support, meet ups, hackathons, etc.\n",
    "\n",
    "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
    "\n",
    "### **Main Use Cases**\n",
    "\n",
    "* **Summarization** - Express the most important facts about a body of text or chat interaction\n",
    "* **Question and Answering Over Documents** - Use information held within documents to answer questions or query\n",
    "* **Extraction** - Pull structured data from a body of text or an user query(ç»“æ„åŒ–æ•°æ®æå–)\n",
    "* **Evaluation** - Understand the quality of output from your application\n",
    "* **Querying Tabular Data** - Pull data from databases or other tabular source\n",
    "* **Code Understanding** - Reason about and digest code\n",
    "* **Interacting with APIs** - Query APIs and interact with the outside world\n",
    "* **Chatbots** - A framework to have a back and forth interaction with a user combined with memory in a chat interface\n",
    "* **Agents** - Use LLMs to make decisions about what to do next. Enable these decisions with tools.\n",
    "\n",
    "Want to see live examples of these use cases? Head over to the [LangChain Project Gallery](https://github.com/gkamradt/langchain-tutorials)\n",
    "\n",
    "#### **Authors Note:**\n",
    "\n",
    "* This cookbook will not cover all aspects of LangChain. It's contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Technical Documentation](https://python.langchain.com/en/latest/index.html)\n",
    "* This notebook assumes is that you've seen part 1 of this series [Fundamentals](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb). This notebook is focused on what to do and how to apply those fundamentals.\n",
    "* You'll notice I repeat import statements throughout the notebook. My intention is to lean on the side of clarity and help you see the full code block in one spot. No need to go back and forth to see when we imported a package.\n",
    "* We use the default models throughout the notebook, at the time of writing they were davinci-003 and gpt-3.5-turbo. You would no doubt get better results with GPT4\n",
    "\n",
    "Let's get started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e323fb6",
   "metadata": {},
   "source": [
    "Throughout this tutorial we will use OpenAI's various [models](https://platform.openai.com/docs/models/overview). LangChain makes it easy to [subsistute LLMs](https://langchain.com/integrations.html#:~:text=integrations%20LangChain%20provides.-,LLMs,-LLM%20Provider) so you can BYO-LLM if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9815081",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from apikey import apikey, serpapi_key\n",
    "\n",
    "openai_api_key = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd3587c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell if you want to make your display wider\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {},
   "source": [
    "# LangChain Use Cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bbdb1dc",
   "metadata": {},
   "source": [
    "## Case1ï¼šSummarization\n",
    "\n",
    "One of the most common use cases for LangChain and LLMs is summarization. You can summarize any piece of text, but use cases span from summarizing calls, articles, books, academic papers, legal documents, user history, a table, or financial documents. It's super helpful to have a tool which can summarize information quickly.\n",
    "\n",
    "* **Deep Dive** - (Coming Soon)\n",
    "* **Examples** - [Summarizing B2B Sales Calls](https://www.youtube.com/watch?v=DIw4rbpI9ic)\n",
    "* **Use Cases** - Summarize Articles, Transcripts, Chat History, Slack/Discord, Customer Interactions, Medical Papers, Legal Documents, Podcasts, Tweet Threads, Code Bases, Product Reviews, Financial Documents\n",
    "\n",
    "### Summaries Of Short Text\n",
    "\n",
    "For summaries of short texts, the method is straightforward, in fact you don't need to do anything fancy other than simple prompting with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c292592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' but I call it out here explicitly so you know where to change it later if you want\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "# Create our template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f539cb53",
   "metadata": {},
   "source": [
    "Let's let's find a confusing text online. *[Source](https://www.smithsonianmag.com/smart-news/long-before-trees-overtook-the-land-earth-was-covered-by-giant-mushrooms-13709647/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df2cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusing_text = \"\"\"\n",
    "For the next 130 years, debate raged.\n",
    "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
    "â€œThe problem is that when you look up close at the anatomy, itâ€™s evocative of a lot of different things, but itâ€™s diagnostic of nothing,â€ says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
    "â€œAnd itâ€™s so damn big that when whenever someone says itâ€™s something, everyone elseâ€™s hackles get up: â€˜How could you have a lichen 20 feet tall?â€™â€\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03d31842",
   "metadata": {},
   "source": [
    "Let's take a look at what prompt will be sent to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406eb8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Prompt Begin -------\n",
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Respond in a manner that a 5 year old would understand.\n",
      "\n",
      "%TEXT:\n",
      "\n",
      "For the next 130 years, debate raged.\n",
      "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
      "â€œThe problem is that when you look up close at the anatomy, itâ€™s evocative of a lot of different things, but itâ€™s diagnostic of nothing,â€ says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
      "â€œAnd itâ€™s so damn big that when whenever someone says itâ€™s something, everyone elseâ€™s hackles get up: â€˜How could you have a lichen 20 feet tall?â€™â€\n",
      "\n",
      "\n",
      "------- Prompt End -------\n"
     ]
    }
   ],
   "source": [
    "print (\"------- Prompt Begin -------\")\n",
    "\n",
    "final_prompt = prompt.format(text=confusing_text)\n",
    "print(final_prompt)\n",
    "\n",
    "print (\"------- Prompt End -------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a95e53d9",
   "metadata": {},
   "source": [
    "Finally let's pass it through the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7e4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For 130 years, people argued about what Prototaxites was. Some thought it was a lichen, some thought it was a fungus, and some thought it was a tree. But no one could agree. It was so big that it was hard to figure out what it was.\n"
     ]
    }
   ],
   "source": [
    "output = llm(final_prompt)\n",
    "print (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "751c6359",
   "metadata": {},
   "source": [
    "This method works fine, but for longer text, it can become a pain to manage and you'll run into token limits. Luckily LangChain has out of the box support for different methods to summarize via their [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html).\n",
    "\n",
    "### Summaries Of Longer Text\n",
    "\n",
    "*Note: This method will also work for short text too*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3441484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95b575c",
   "metadata": {},
   "source": [
    "Let's load up a longer document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c33f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2008(This essay is derived from a talk at the 2008 Startup School.)About a month after we started Y Combinator we came up with the\n",
      "phrase that became our motto: Make something people want.  We've\n",
      "learned a lot since then, but if I were choosing now that's still\n",
      "the one I'd pick.\n"
     ]
    }
   ],
   "source": [
    "with open('data/PaulGrahamEssays/good.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Printing the first 285 characters as a preview\n",
    "print (text[:285])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b489d2a2",
   "metadata": {},
   "source": [
    "Then let's check how many tokens are in this document. [get_num_tokens](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI.get_num_tokens) is a nice method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0e8181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3970 tokens in your file\n"
     ]
    }
   ],
   "source": [
    "num_tokens = llm.get_num_tokens(text)\n",
    "\n",
    "print (f\"There are {num_tokens} tokens in your file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf8eda6",
   "metadata": {},
   "source": [
    "While you could likely stuff this text in your prompt, let's act like it's too big and needs another method.\n",
    "\n",
    "First we'll need to split it up. This process is called 'chunking' or 'splitting' your text into smaller pieces. I like the [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html) because it's easy to control but there are a [bunch](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) you can try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25dd80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You now have 4 docs intead of 1 piece of text\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=5000, chunk_overlap=350)\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")\n",
    "\n",
    "# ccï¼š\n",
    "# chunk_size å’Œ chunk_overlap æ˜¯ RecursiveCharacterTextSplitter ç±»çš„ä¸¤ä¸ªå‚æ•°ã€‚\n",
    "# chunk_size è¡¨ç¤ºæ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œè€Œ chunk_overlap è¡¨ç¤ºç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ã€‚\n",
    "# åœ¨ä½¿ç”¨ RecursiveCharacterTextSplitter å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å‰²æ—¶ï¼Œæ–‡æœ¬å°†è¢«åˆ†æˆå¤šä¸ªå—ï¼Œæ¯ä¸ªå—çš„å¤§å°ä¸º chunk_sizeï¼Œç›¸é‚»å—ä¹‹é—´å°†æœ‰ chunk_overlap ä¸ªå­—ç¬¦é‡å ã€‚\n",
    "# è¿™äº›å‚æ•°çš„å€¼å¯ä»¥æ ¹æ®å…·ä½“çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„åˆ†å‰²æ•ˆæœ ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e7547a3",
   "metadata": {},
   "source": [
    "Next we need to load up a chain which will make successive calls to the LLM for us. Want to see the prompt being used in the chain below? Check out the [LangChain documentation](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py)\n",
    "\n",
    "For information on the difference between chain types, check out this video on [token limit workarounds](https://youtu.be/f9_BWhCI4Zo)\n",
    "\n",
    "*Note: You could also get fancy and make the first 4 calls of the map_reduce run in parallel too*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ddd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your chain ready to use\n",
    "chain = load_summarize_chain(llm=llm, chain_type='map_reduce') # verbose=True optional to see what is getting sent to the LLM\n",
    "\n",
    "# ccï¼šchain_typeå‚æ•°ç”¨äºæŒ‡å®šä½¿ç”¨å“ªç§ç±»å‹çš„é“¾æ¥è¿›è¡Œæ–‡æœ¬æ‘˜è¦ã€‚\n",
    "# åœ¨è¿™ä¸ªç‰¹å®šçš„ä»£ç ä¸­ï¼Œchain_typeè¢«è®¾ç½®ä¸ºmap_reduceï¼Œè¿™æ„å‘³ç€ä½¿ç”¨MapReduceç®—æ³•æ¥ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ã€‚\n",
    "# MapReduceæ˜¯ä¸€ç§ç”¨äºå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„ç¼–ç¨‹æ¨¡å‹ï¼Œå®ƒå°†æ•°æ®åˆ†æˆå°å—å¹¶åœ¨å¤šä¸ªè®¡ç®—æœºä¸Šå¹¶è¡Œå¤„ç†ï¼Œæœ€åå°†ç»“æœåˆå¹¶åœ¨ä¸€èµ·ã€‚\n",
    "# å› æ­¤ï¼Œä½¿ç”¨map_reduceç±»å‹çš„é“¾å¯ä»¥æé«˜æ–‡æœ¬æ‘˜è¦çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be0b2d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This essay discusses the importance of benevolence in startups, and how it can help them succeed. It explains how benevolence can improve morale, make people want to help, and help startups be decisive. It also explains how benevolence can help startups gain investor interest and make them hard to kill. It argues that markets can learn to value potential earnings and that starting a company with benevolent aims is currently undervalued.\n"
     ]
    }
   ],
   "source": [
    "# Use it. This will run through the 4 documents, summarize the chunks, then get a summary of the summary.\n",
    "output = chain.run(docs)\n",
    "print (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2d664fc",
   "metadata": {},
   "source": [
    "## Case2ï¼šQuestion & Answering Using Documents As Context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad87c72b",
   "metadata": {},
   "source": [
    "*[LangChain Question & Answer Docs](https://python.langchain.com/en/latest/use_cases/question_answering.html)*\n",
    "\n",
    "In order to use LLMs for question and answer we must:\n",
    "\n",
    "1. Pass the LLM relevant context it needs to answer a question\n",
    "2. Pass it our question that we want answered\n",
    "\n",
    "Simplified, this process looks like this \"llm(your context + your question) = your answer\"\n",
    "\n",
    "* **Deep Dive** - [Question A Book](https://youtu.be/h0DHDp1FbmQ), [Ask Questions To Your Custom Files](https://youtu.be/EnT-ZTrcPrg), [Chat Your Data JS (1000 pages of Financial Reports)](https://www.youtube.com/watch?v=Ix9WIZpArm0&t=1051s), [LangChain Q&A webinar](https://www.crowdcast.io/c/rh66hcwivly0)\n",
    "* **Examples** - [ChatPDF](https://www.chatpdf.com/)\n",
    "* **Use Cases** - Chat your documents, ask questions to academic papers, create study guides, reference medical information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "685e15f3",
   "metadata": {},
   "source": [
    "### Simple Q&A Example\n",
    "\n",
    "Here let's review the convention of `llm(your context + your question) = your answer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ebd8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4795187",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Rachel is 30 years old\n",
    "Bob is 45 years old\n",
    "Kevin is 65 years old\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who is under 40 years old?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2184b11b",
   "metadata": {},
   "source": [
    "Then combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c53650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel is under 40 years old.\n"
     ]
    }
   ],
   "source": [
    "output = llm(context + question)\n",
    "\n",
    "# I strip the text to remove the leading and trailing whitespace\n",
    "print (output.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "385180ca",
   "metadata": {},
   "source": [
    "As we ramp up our sophistication, we'll take advantage of this convention more.\n",
    "\n",
    "The hard part comes in when you need to be selective about *which* data you put in your context. This field of study is called \"[document retrieval](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\" and tightly coupled with AI Memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53ed4080",
   "metadata": {},
   "source": [
    "### Using Embeddings\n",
    "\n",
    "I informally call what were about to go through as \"The VectorStore Dance\". It's the process of splitting your text, embedding the chunks, putting the embeddings in a DB, and then querying them. For a full video on this check out [How To Question A Book](https://www.youtube.com/watch?v=h0DHDp1FbmQ)\n",
    "\n",
    "The goal is to select relevant chunks of our long text, but which chunks do we pull? The most popular method is to pull *similar* texts based off comparing vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7a02ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "# The vectorstore we'll be using\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# The easy document loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The embedding engine that will convert our text to vectors\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40afcfec",
   "metadata": {},
   "source": [
    "Let's load up a longer document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5772bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 74663 characters in that document\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "doc = loader.load()\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb87424c",
   "metadata": {},
   "source": [
    "Now let's split our long doc into smaller pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4a6e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# ccï¼šcreate_documents æ–¹æ³•æ˜¯å°†ä¸€ä¸ªæ–‡æœ¬åˆ†æˆå¤šä¸ªæ–‡æ¡£ï¼Œè€Œ split_documents æ–¹æ³•æ˜¯å°†ä¸€ä¸ªæ–‡æ¡£åˆ†æˆå¤šä¸ªå­æ–‡æ¡£ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "723e8aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 29 documents that have an average of 2,930 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b591198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1b13348",
   "metadata": {},
   "source": [
    "Create your retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47cd969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6aa2963c",
   "metadata": {},
   "source": [
    "Now it's time to ask a question. The retriever will go get the similar documents and combine with your question for the LLM to reason through.\n",
    "\n",
    "Note: It may not seem like much, but the magic here is that we didn't have to pass in our full original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a062c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The author describes painting as good work.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does the author describe as good work?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11f0f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The theme of this article is the importance of working on things that are not prestigious.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cc\n",
    "query = \"What is the theme of this article?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be503d53",
   "metadata": {},
   "source": [
    "If you wanted to do more you would hook this up to a cloud vector database, use a tool like metal and start managing your documents, with external data sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3d04dc9",
   "metadata": {},
   "source": [
    "## Case3ï¼šExtraction\n",
    "*[LangChain Extraction Docs](https://python.langchain.com/en/latest/use_cases/extraction.html)*\n",
    "\n",
    "Extraction is the process of parsing data from a piece of text. This is commonly used with output parsing in order to *structure* our data.\n",
    "\n",
    "* **Deep Dive** - [Use LLMs to Extract Data From Text (Expert Level Text Extraction](https://youtu.be/xZzvwR9jdPA), [Structured Output From OpenAI (Clean Dirty Data)](https://youtu.be/KwAXfey-xQk)\n",
    "* **Examples** - [OpeningAttributes](https://twitter.com/GregKamradt/status/1646500373837008897)\n",
    "* **Use Cases:** Extract a structured row from a sentence to insert into a database, extract multiple rows from a long document to insert into a database, extracting parameters from a user query to make an API call\n",
    "\n",
    "A popular library for extraction is [Kor](https://eyurtsev.github.io/kor/). We won't cover it today but I highly suggest checking it out for advanced extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "904d43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6923ca8b",
   "metadata": {},
   "source": [
    "### Vanilla Extraction\n",
    "\n",
    "Let's start off with an easy example. Here I simply supply a prompt with instructions with the type of output I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab1cce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f16ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 'ğŸ', 'Pear': 'ğŸ', 'kiwi': 'ğŸ¥'}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "\n",
    "print (output.content)\n",
    "print (type(output.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39d6cff3",
   "metadata": {},
   "source": [
    "Let's turn this into a proper python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "314286b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 'ğŸ', 'Pear': 'ğŸ', 'kiwi': 'ğŸ¥'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))\n",
    "\n",
    "# eval() å†…ç½®å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²è§£æä¸ºpythonè¡¨è¾¾å¼ï¼Œå¹¶è¿”å›å…¶ç»“æœã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3909eb29",
   "metadata": {},
   "source": [
    "While this worked this time, it's not a long term reliable method for more advanced use cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6a0a90d",
   "metadata": {},
   "source": [
    "### Using LangChain's Response Schema\n",
    "\n",
    "LangChain's response schema will does two things for us: \n",
    "\n",
    "1. Autogenerate the a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
    "\n",
    "2. Read the output from the LLM and turn it into a proper python object for me\n",
    "\n",
    "Here I define the schema I want. I'm going to pull out the song and artist that a user wants to play from a pseudo chat message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc2ba0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema I want out\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9e3c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "\n",
    "# ccï¼šé€šè¿‡è¿™ç§æ–¹å¼ç›´æ¥ç”Ÿæˆäº†jsonï¼Œè€Œä¸éœ€è¦æˆ‘ä»¬ç‰¹åˆ«å†è¿›è¡Œå¼ºè°ƒç”Ÿæˆjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d702900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb6adde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "print (fruit_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8664302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "fruit_output = chat_model(fruit_query.to_messages())\n",
    "output = output_parser.parse(fruit_output.content)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b68b8eeb",
   "metadata": {},
   "source": [
    "Awesome, now we have a dictionary that we can use later down the line\n",
    "\n",
    "<span style=\"background:#fff5d6\">Warning:</span> The parser looks for an output from the LLM in a specific format. Your model may not output the same format every time. Make sure to handle errors with this one. GPT4 and future iterations will be more reliable.\n",
    "\n",
    "For more advanced parsing check out [Kor](https://eyurtsev.github.io/kor/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2fb4ba6",
   "metadata": {},
   "source": [
    "## Case4ï¼šEvaluationï¼šæ£€æŸ¥&è¯„ä¼°å¤§æ¨¡å‹è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ‡å‡†\n",
    "\n",
    "*[LangChain Evaluation Docs](https://python.langchain.com/en/latest/use_cases/evaluation.html)*\n",
    "\n",
    "Evaluation is the process of doing quality checks on the output of your applications. Normal, deterministic, code has tests we can run, but judging the output of LLMs is more difficult because of the unpredictableness and variability of natural language. LangChain provides tools that aid us in this journey.\n",
    "\n",
    "* **Deep Dive** - Coming Soon\n",
    "* **Examples** - [Lance Martin's Advaned](https://twitter.com/RLanceMartin) [Auto-Evaluator](https://github.com/rlancemartin/auto-evaluator)\n",
    "* **Use Cases:** Run quality checks on your summarization or Question & Answer pipelines, check the output of you summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fbaa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings, store, and retrieval\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Model and doc loader\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Eval!\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f35fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 74663 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Our long essay from before\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7acca7da",
   "metadata": {},
   "source": [
    "First let's do the Vectorestore dance so we can do question and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1955faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 29 documents that have an average of 2,930 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "890b85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and docstore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a0d6e25",
   "metadata": {},
   "source": [
    "Make your retrieval chain. Notice how I have an `input_key` parameter now. This tells the chain which key from a dictionary I supply has my prompt/query in it. I specify `question` to match the question in the dict below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddb3f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), input_key=\"question\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37dd0cd",
   "metadata": {},
   "source": [
    "Now I'll pass a list of questions and ground truth answers to the LLM that I know are correct (I validated them as a human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d93d08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"Which company sold the microcomputer kit that his friend built himself?\", 'answer' : 'Healthkit'},\n",
    "    {'question' : \"What was the small city he talked about in the city that is the financial capital of USA?\", 'answer' : 'Yorkville, NY'}\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c4b591",
   "metadata": {},
   "source": [
    "I'll use `chain.apply` to run both my questions one by one separately.\n",
    "\n",
    "One of the cool parts is that I'll get my list of question and answers dictionaries back, but there'll be another key in the dictionary `result` which will be the output from the LLM.\n",
    "\n",
    "Note: I specifically made my 2nd question ambigious and tough to answer in one pass so the LLM would get it incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4a4e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/text-embedding-ada-002/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7aa8cc86d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which company sold the microcomputer kit that his friend built himself?',\n",
       "  'answer': 'Healthkit',\n",
       "  'result': ' The microcomputer kit was sold by Heathkit.'},\n",
       " {'question': 'What was the small city he talked about in the city that is the financial capital of USA?',\n",
       "  'answer': 'Yorkville, NY',\n",
       "  'result': ' The small city he talked about is New York City, the financial capital of the USA.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = chain.apply(question_answers)\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed1226c9",
   "metadata": {},
   "source": [
    "We then have the LLM compare my ground truth answer (the `answer` key) with the result from the LLM (`result` key).\n",
    "\n",
    "Or simply, we are asking the LLM to grade itself. What a wild world we live in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae119b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     predictions,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2882750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' CORRECT'}, {'text': ' INCORRECT'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs\n",
    "\n",
    "# ccï¼šè¿™é‡Œæœ‰ç‚¹ç–‘é—®ï¼šç¬¬ä¸€ä¸ªé—®é¢˜ä¸æ˜¯æ‹¼å†™é”™äº†ä¹ˆï¼Ÿ\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b30268b",
   "metadata": {},
   "source": [
    "This is correct! Notice how the answer in question #1 was \"Healthkit\" and the prediction was \"The microcomputer kit was sold by Heathkit.\" The LLM knew that the answer and result were the same and gave us a \"correct\" label. Awesome.\n",
    "\n",
    "For #2 it knew they were not the same and gave us an \"incorrect\" label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2745752",
   "metadata": {},
   "source": [
    "## Case5ï¼šQuerying Tabular Data\n",
    "\n",
    "*[LangChain Querying Tabular Data Docs](https://python.langchain.com/en/latest/use_cases/tabular.html)*\n",
    "\n",
    "The most common type of data in the world sits in tabular form (ok, ok, besides unstructured data). It is super powerful to be able to query this data with LangChain and pass it through to an LLM \n",
    "\n",
    "* **Deep Dive** - Coming Soon\n",
    "* **Examples** - TBD\n",
    "* **Use Cases:** Use LLMs to query data about users, do data analysis, get real time information from your DBs\n",
    "\n",
    "For futher reading check out \"Agents + Tabular Data\" ([Pandas](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/pandas.html), [SQL](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html), [CSV](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/csv.html))\n",
    "\n",
    "Let's query an SQLite DB with natural language. We'll look at the [San Francisco Trees](https://data.sfgov.org/City-Infrastructure/Street-Tree-List/tkzw-k3nq) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b19c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "294a4e7f",
   "metadata": {},
   "source": [
    "We'll start off by specifying where our data is and get the connection ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6044d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db_path = 'data/San_Francisco_Trees.db'\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "203eedd4",
   "metadata": {},
   "source": [
    "Then we'll create a chain that take our LLM, and DB. I'm setting `verbose=True` so you can see what is happening underneath the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dccf0957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matao/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chains/sql_database/base.py:63: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.run(\"How many Species of trees are there in San Francisco?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e045a50",
   "metadata": {},
   "source": [
    "This is awesome! There are actually a few steps going on here.\n",
    "\n",
    "**Steps:**\n",
    "1. Find which table to use\n",
    "2. Find which column to use\n",
    "3. Construct the correct sql query\n",
    "4. Execute that query\n",
    "5. Get the result\n",
    "6. Return a natural language reponse back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b96fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccï¼š\n",
    "db_chain.run(\"How many plant type are there in San Francisco? and what are these?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f2bcbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many tables are there in this database? and list them.\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) FROM sqlite_master WHERE type='table';\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(1,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThere is 1 table in this database, named SFTrees.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There is 1 table in this database, named SFTrees.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ccï¼š\n",
    "db_chain.run(\"How many tables are there in this database? and list them.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd61598",
   "metadata": {},
   "source": [
    "Let's confirm via pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "299ff6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "connection = sqlite3.connect(sqlite_db_path)\n",
    "\n",
    "# Define your SQL query\n",
    "query = \"SELECT count(distinct qSpecies) FROM SFTrees\"\n",
    "\n",
    "# Read the SQL query into a Pandas DataFrame\n",
    "df = pd.read_sql_query(query, connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1b2dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n"
     ]
    }
   ],
   "source": [
    "# Display the result in the first column first cell\n",
    "print(df.iloc[0,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c48b5a42",
   "metadata": {},
   "source": [
    "Nice! The answers match."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04293535",
   "metadata": {},
   "source": [
    "## Case6ï¼šCode Understanding\n",
    "\n",
    "*[LangChain Code Understanding Docs](https://python.langchain.com/en/latest/use_cases/code.html)*\n",
    "\n",
    "One of the most exciting abilities of LLMs is code undestanding. People around the world are leveling up their output in both speed & quality due to AI help. A big part of this is having a LLM that can understand code and help you with a particular task.\n",
    "\n",
    "* **Deep Dive** - Coming Soon\n",
    "* **Examples** - TBD\n",
    "* **Use Cases:** Co-Pilot-esque functionality that can help answer questions from a specific library, help you generate new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3101c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to read local files\n",
    "import os\n",
    "\n",
    "# Vector Support\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Model and chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Text splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c4dfd6d",
   "metadata": {},
   "source": [
    "We will do the Vectorstore dance again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a9247e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=(), openai_api_key=openai_api_key)\n",
    "\n",
    "# ccï¼šdisallowed_special å‚æ•°çš„ä½œç”¨æ˜¯ç¡®ä¿åœ¨è®¡ç®—æ–‡æœ¬åµŒå…¥å‘é‡æ—¶ï¼Œä¸ä¼šè®²è¿™äº›ç‰¹æ®Šå­—ç¬¦è§†ä¸ºæœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf12eb2c",
   "metadata": {},
   "source": [
    "I put a small python package [The Fuzz](https://github.com/seatgeek/thefuzz) (personal indie favorite) in the data folder of this repo.\n",
    "\n",
    "The loop below will go through each file in the library and load it up as a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd3973a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'data/thefuzz'\n",
    "docs = []\n",
    "\n",
    "# Go through each folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    \n",
    "    # Go through each file\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            # Load up the file as a doc and split\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52bf2a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import unittest\\nimport re\\nimport pycodestyle\\n\\nfrom thefuzz import fuzz\\nfrom thefuzz import process\\nfrom thefuzz import utils\\nfrom thefuzz.string_processing import StringProcessor\\n\\n\\nclass StringProcessingTest(unittest.TestCase):\\n    def test_replace_non_letters_non_numbers_with_whitespace(self):\\n        strings = [\"new york mets - atlanta braves\", \"CÃ£es danados\",\\n                   \"New York //// Mets $$$\", \"Ã‡a va?\"]\\n        for string in strings:\\n            proc_string = StringProcessor.replace_non_letters_non_numbers_with_whitespace(string)\\n            regex = re.compile(r\"(?ui)[\\\\W]\")\\n            for expr in regex.finditer(proc_string):\\n                self.assertEqual(expr.group(), \" \")\\n\\n    def test_dont_condense_whitespace(self):\\n        s1 = \"new york mets - atlanta braves\"\\n        s2 = \"new york mets atlanta braves\"\\n        p1 = StringProcessor.replace_non_letters_non_numbers_with_whitespace(s1)\\n        p2 = StringProcessor.replace_non_letters_non_numbers_with_whitespace(s2)\\n        self.assertNotEqual(p1, p2)\\n\\n\\nclass UtilsTest(unittest.TestCase):\\n    def setUp(self):\\n        self.s1 = \"new york mets\"\\n        self.s1a = \"new york mets\"\\n        self.s2 = \"new YORK mets\"\\n        self.s3 = \"the wonderful new york mets\"\\n        self.s4 = \"new york mets vs atlanta braves\"\\n        self.s5 = \"atlanta braves vs new york mets\"\\n        self.s6 = \"new york mets - atlanta braves\"\\n        self.mixed_strings = [\\n            \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\",\\n            \"C\\'est la vie\",\\n            \"Ã‡a va?\",\\n            \"CÃ£es danados\",\\n            \"\\\\xacCamarÃµes assados\",\\n            \"a\\\\xac\\\\u1234\\\\u20ac\\\\U00008000\",\\n            \"\\\\u00C1\"\\n        ]\\n\\n    def tearDown(self):\\n        pass\\n\\n    def test_ascii_only(self):\\n        for s in self.mixed_strings:\\n            utils.ascii_only(s)\\n\\n    def test_fullProcess(self):\\n        for s in self.mixed_strings:\\n            utils.full_process(s)\\n\\n    def test_fullProcessForceAscii(self):\\n        for s in self.mixed_strings:\\n            utils.full_process(s, force_ascii=True)\\n\\n\\nclass RatioTest(unittest.TestCase):\\n\\n    def setUp(self):\\n        self.s1 = \"new york mets\"\\n        self.s1a = \"new york mets\"\\n        self.s2 = \"new YORK mets\"\\n        self.s3 = \"the wonderful new york mets\"\\n        self.s4 = \"new york mets vs atlanta braves\"\\n        self.s5 = \"atlanta braves vs new york mets\"\\n        self.s6 = \"new york mets - atlanta braves\"\\n        self.s7 = \\'new york city mets - atlanta braves\\'\\n        # test silly corner cases\\n        self.s8 = \\'{\\'\\n        self.s8a = \\'{\\'\\n        self.s9 = \\'{a\\'\\n        self.s9a = \\'{a\\'\\n        self.s10 = \\'a{\\'\\n        self.s10a = \\'{b\\'\\n\\n        self.cirque_strings = [\\n            \"cirque du soleil - zarkana - las vegas\",\\n            \"cirque du soleil \",\\n            \"cirque du soleil las vegas\",\\n            \"zarkana las vegas\",\\n            \"las vegas cirque du soleil at the bellagio\",\\n            \"zarakana - cirque du soleil - bellagio\"\\n        ]\\n\\n        self.baseball_strings = [\\n            \"new york mets vs chicago cubs\",\\n            \"chicago cubs vs chicago white sox\",\\n            \"philladelphia phillies vs atlanta braves\",\\n            \"braves vs mets\",\\n        ]\\n\\n    def tearDown(self):\\n        pass\\n\\n    def testEqual(self):\\n        self.assertEqual(fuzz.ratio(self.s1, self.s1a), 100)\\n        self.assertEqual(fuzz.ratio(self.s8, self.s8a), 100)\\n        self.assertEqual(fuzz.ratio(self.s9, self.s9a), 100)\\n\\n    def testCaseInsensitive(self):\\n        self.assertNotEqual(fuzz.ratio(self.s1, self.s2), 100)\\n        self.assertEqual(fuzz.ratio(utils.full_process(self.s1), utils.full_process(self.s2)), 100)\\n\\n    def testPartialRatio(self):\\n        self.assertEqual(fuzz.partial_ratio(self.s1, self.s3), 100)\\n\\n    def testTokenSortRatio(self):\\n        self.assertEqual(fuzz.token_sort_ratio(self.s1, self.s1a), 100)', metadata={'source': 'data/thefuzz/test_thefuzz.py'}),\n",
       " Document(page_content='def testTokenSortRatio(self):\\n        self.assertEqual(fuzz.token_sort_ratio(self.s1, self.s1a), 100)\\n\\n    def testPartialTokenSortRatio(self):\\n        self.assertEqual(fuzz.partial_token_sort_ratio(self.s1, self.s1a), 100)\\n        self.assertEqual(fuzz.partial_token_sort_ratio(self.s4, self.s5), 100)\\n        self.assertEqual(fuzz.partial_token_sort_ratio(self.s8, self.s8a, full_process=False), 100)\\n        self.assertEqual(fuzz.partial_token_sort_ratio(self.s9, self.s9a, full_process=True), 100)\\n        self.assertEqual(fuzz.partial_token_sort_ratio(self.s9, self.s9a, full_process=False), 100)\\n        self.assertEqual(fuzz.partial_token_sort_ratio(self.s10, self.s10a, full_process=False), 50)\\n\\n    def testTokenSetRatio(self):\\n        self.assertEqual(fuzz.token_set_ratio(self.s4, self.s5), 100)\\n        self.assertEqual(fuzz.token_set_ratio(self.s8, self.s8a, full_process=False), 100)\\n        self.assertEqual(fuzz.token_set_ratio(self.s9, self.s9a, full_process=True), 100)\\n        self.assertEqual(fuzz.token_set_ratio(self.s9, self.s9a, full_process=False), 100)\\n        self.assertEqual(fuzz.token_set_ratio(self.s10, self.s10a, full_process=False), 50)\\n\\n    def testPartialTokenSetRatio(self):\\n        self.assertEqual(fuzz.partial_token_set_ratio(self.s4, self.s7), 100)\\n\\n    def testQuickRatioEqual(self):\\n        self.assertEqual(fuzz.QRatio(self.s1, self.s1a), 100)\\n\\n    def testQuickRatioCaseInsensitive(self):\\n        self.assertEqual(fuzz.QRatio(self.s1, self.s2), 100)\\n\\n    def testQuickRatioNotEqual(self):\\n        self.assertNotEqual(fuzz.QRatio(self.s1, self.s3), 100)\\n\\n    def testWRatioEqual(self):\\n        self.assertEqual(fuzz.WRatio(self.s1, self.s1a), 100)\\n\\n    def testWRatioCaseInsensitive(self):\\n        self.assertEqual(fuzz.WRatio(self.s1, self.s2), 100)\\n\\n    def testWRatioPartialMatch(self):\\n        # a partial match is scaled by .9\\n        self.assertEqual(fuzz.WRatio(self.s1, self.s3), 90)\\n\\n    def testWRatioMisorderedMatch(self):\\n        # misordered full matches are scaled by .95\\n        self.assertEqual(fuzz.WRatio(self.s4, self.s5), 95)\\n\\n    def testWRatioStr(self):\\n        self.assertEqual(fuzz.WRatio(str(self.s1), str(self.s1a)), 100)\\n\\n    def testQRatioStr(self):\\n        self.assertEqual(fuzz.WRatio(str(self.s1), str(self.s1a)), 100)\\n\\n    def testEmptyStringsScore100(self):\\n        self.assertEqual(fuzz.ratio(\"\", \"\"), 100)\\n        self.assertEqual(fuzz.partial_ratio(\"\", \"\"), 100)\\n\\n    def testIssueSeven(self):\\n        s1 = \"HSINCHUANG\"\\n        s2 = \"SINJHUAN\"\\n        s3 = \"LSINJHUANG DISTRIC\"\\n        s4 = \"SINJHUANG DISTRICT\"\\n\\n        self.assertGreater(fuzz.partial_ratio(s1, s2), 75)\\n        self.assertGreater(fuzz.partial_ratio(s1, s3), 75)\\n        self.assertGreater(fuzz.partial_ratio(s1, s4), 75)\\n\\n    def testRatioUnicodeString(self):\\n        s1 = \"\\\\u00C1\"\\n        s2 = \"ABCD\"\\n        score = fuzz.ratio(s1, s2)\\n        self.assertEqual(0, score)\\n\\n    def testPartialRatioUnicodeString(self):\\n        s1 = \"\\\\u00C1\"\\n        s2 = \"ABCD\"\\n        score = fuzz.partial_ratio(s1, s2)\\n        self.assertEqual(0, score)\\n\\n    def testWRatioUnicodeString(self):\\n        s1 = \"\\\\u00C1\"\\n        s2 = \"ABCD\"\\n        score = fuzz.WRatio(s1, s2)\\n        self.assertEqual(0, score)\\n\\n        # Cyrillic.\\n        s1 = \"\\\\u043f\\\\u0441\\\\u0438\\\\u0445\\\\u043e\\\\u043b\\\\u043e\\\\u0433\"\\n        s2 = \"\\\\u043f\\\\u0441\\\\u0438\\\\u0445\\\\u043e\\\\u0442\\\\u0435\\\\u0440\\\\u0430\\\\u043f\\\\u0435\\\\u0432\\\\u0442\"\\n        score = fuzz.WRatio(s1, s2, force_ascii=False)\\n        self.assertNotEqual(0, score)\\n\\n        # Chinese.\\n        s1 = \"\\\\u6211\\\\u4e86\\\\u89e3\\\\u6570\\\\u5b66\"\\n        s2 = \"\\\\u6211\\\\u5b66\\\\u6570\\\\u5b66\"\\n        score = fuzz.WRatio(s1, s2, force_ascii=False)\\n        self.assertNotEqual(0, score)\\n\\n    def testQRatioUnicodeString(self):\\n        s1 = \"\\\\u00C1\"\\n        s2 = \"ABCD\"\\n        score = fuzz.QRatio(s1, s2)\\n        self.assertEqual(0, score)', metadata={'source': 'data/thefuzz/test_thefuzz.py'}),\n",
       " Document(page_content='def testQRatioUnicodeString(self):\\n        s1 = \"\\\\u00C1\"\\n        s2 = \"ABCD\"\\n        score = fuzz.QRatio(s1, s2)\\n        self.assertEqual(0, score)\\n\\n        # Cyrillic.\\n        s1 = \"\\\\u043f\\\\u0441\\\\u0438\\\\u0445\\\\u043e\\\\u043b\\\\u043e\\\\u0433\"\\n        s2 = \"\\\\u043f\\\\u0441\\\\u0438\\\\u0445\\\\u043e\\\\u0442\\\\u0435\\\\u0440\\\\u0430\\\\u043f\\\\u0435\\\\u0432\\\\u0442\"\\n        score = fuzz.QRatio(s1, s2, force_ascii=False)\\n        self.assertNotEqual(0, score)\\n\\n        # Chinese.\\n        s1 = \"\\\\u6211\\\\u4e86\\\\u89e3\\\\u6570\\\\u5b66\"\\n        s2 = \"\\\\u6211\\\\u5b66\\\\u6570\\\\u5b66\"\\n        score = fuzz.QRatio(s1, s2, force_ascii=False)\\n        self.assertNotEqual(0, score)\\n\\n    def testQratioForceAscii(self):\\n        s1 = \"ABCD\\\\u00C1\"\\n        s2 = \"ABCD\"\\n\\n        score = fuzz.QRatio(s1, s2, force_ascii=True)\\n        self.assertEqual(score, 100)\\n\\n        score = fuzz.QRatio(s1, s2, force_ascii=False)\\n        self.assertLess(score, 100)\\n\\n    def testQRatioForceAscii(self):\\n        s1 = \"ABCD\\\\u00C1\"\\n        s2 = \"ABCD\"\\n\\n        score = fuzz.WRatio(s1, s2, force_ascii=True)\\n        self.assertEqual(score, 100)\\n\\n        score = fuzz.WRatio(s1, s2, force_ascii=False)\\n        self.assertLess(score, 100)\\n\\n    def testTokenSetForceAscii(self):\\n        s1 = \"ABCD\\\\u00C1 HELP\\\\u00C1\"\\n        s2 = \"ABCD HELP\"\\n\\n        score = fuzz._token_set(s1, s2, force_ascii=True)\\n        self.assertEqual(score, 100)\\n\\n        score = fuzz._token_set(s1, s2, force_ascii=False)\\n        self.assertLess(score, 100)\\n\\n    def testTokenSortForceAscii(self):\\n        s1 = \"ABCD\\\\u00C1 HELP\\\\u00C1\"\\n        s2 = \"ABCD HELP\"\\n\\n        score = fuzz._token_sort(s1, s2, force_ascii=True)\\n        self.assertEqual(score, 100)\\n\\n        score = fuzz._token_sort(s1, s2, force_ascii=False)\\n        self.assertLess(score, 100)\\n\\n\\nclass ValidatorTest(unittest.TestCase):\\n    def setUp(self):\\n        self.testFunc = lambda *args, **kwargs: (args, kwargs)\\n\\n    def testCheckForNone(self):\\n        invalid_input = [\\n            (None, None),\\n            (\\'Some\\', None),\\n            (None, \\'Some\\')\\n        ]\\n        decorated_func = utils.check_for_none(self.testFunc)\\n        for i in invalid_input:\\n            self.assertEqual(decorated_func(*i), 0)\\n\\n        valid_input = (\\'Some\\', \\'Some\\')\\n        actual = decorated_func(*valid_input)\\n        self.assertNotEqual(actual, 0)\\n\\n    def testCheckEmptyString(self):\\n        invalid_input = [\\n            (\\'\\', \\'\\'),\\n            (\\'Some\\', \\'\\'),\\n            (\\'\\', \\'Some\\')\\n        ]\\n        decorated_func = utils.check_empty_string(self.testFunc)\\n        for i in invalid_input:\\n            self.assertEqual(decorated_func(*i), 0)\\n\\n        valid_input = (\\'Some\\', \\'Some\\')\\n        actual = decorated_func(*valid_input)\\n        self.assertNotEqual(actual, 0)\\n\\n\\nclass ProcessTest(unittest.TestCase):\\n\\n    def setUp(self):\\n        self.s1 = \"new york mets\"\\n        self.s1a = \"new york mets\"\\n        self.s2 = \"new YORK mets\"\\n        self.s3 = \"the wonderful new york mets\"\\n        self.s4 = \"new york mets vs atlanta braves\"\\n        self.s5 = \"atlanta braves vs new york mets\"\\n        self.s6 = \"new york mets - atlanta braves\"\\n\\n        self.cirque_strings = [\\n            \"cirque du soleil - zarkana - las vegas\",\\n            \"cirque du soleil \",\\n            \"cirque du soleil las vegas\",\\n            \"zarkana las vegas\",\\n            \"las vegas cirque du soleil at the bellagio\",\\n            \"zarakana - cirque du soleil - bellagio\"\\n        ]\\n\\n        self.baseball_strings = [\\n            \"new york mets vs chicago cubs\",\\n            \"chicago cubs vs chicago white sox\",\\n            \"philladelphia phillies vs atlanta braves\",\\n            \"braves vs mets\",\\n        ]\\n\\n    def testGetBestChoice1(self):\\n        query = \"new york mets at atlanta braves\"\\n        best = process.extractOne(query, self.baseball_strings)\\n        self.assertEqual(best[0], \"braves vs mets\")', metadata={'source': 'data/thefuzz/test_thefuzz.py'}),\n",
       " Document(page_content='def testGetBestChoice1(self):\\n        query = \"new york mets at atlanta braves\"\\n        best = process.extractOne(query, self.baseball_strings)\\n        self.assertEqual(best[0], \"braves vs mets\")\\n\\n    def testGetBestChoice2(self):\\n        query = \"philadelphia phillies at atlanta braves\"\\n        best = process.extractOne(query, self.baseball_strings)\\n        self.assertEqual(best[0], self.baseball_strings[2])\\n\\n    def testGetBestChoice3(self):\\n        query = \"atlanta braves at philadelphia phillies\"\\n        best = process.extractOne(query, self.baseball_strings)\\n        self.assertEqual(best[0], self.baseball_strings[2])\\n\\n    def testGetBestChoice4(self):\\n        query = \"chicago cubs vs new york mets\"\\n        best = process.extractOne(query, self.baseball_strings)\\n        self.assertEqual(best[0], self.baseball_strings[0])\\n\\n    def testWithProcessor(self):\\n        events = [\\n            [\"chicago cubs vs new york mets\", \"CitiField\", \"2011-05-11\", \"8pm\"],\\n            [\"new york yankees vs boston red sox\", \"Fenway Park\", \"2011-05-11\", \"8pm\"],\\n            [\"atlanta braves vs pittsburgh pirates\", \"PNC Park\", \"2011-05-11\", \"8pm\"],\\n        ]\\n        query = [\"new york mets vs chicago cubs\", \"CitiField\", \"2017-03-19\", \"8pm\"],\\n\\n        best = process.extractOne(query, events, processor=lambda event: event[0])\\n        self.assertEqual(best[0], events[0])\\n\\n    def testWithScorer(self):\\n        choices = [\\n            \"new york mets vs chicago cubs\",\\n            \"chicago cubs at new york mets\",\\n            \"atlanta braves vs pittsbugh pirates\",\\n            \"new york yankees vs boston red sox\"\\n        ]\\n\\n        choices_dict = {\\n            1: \"new york mets vs chicago cubs\",\\n            2: \"chicago cubs vs chicago white sox\",\\n            3: \"philladelphia phillies vs atlanta braves\",\\n            4: \"braves vs mets\"\\n        }\\n\\n        # in this hypothetical example we care about ordering, so we use quick ratio\\n        query = \"new york mets at chicago cubs\"\\n        scorer = fuzz.QRatio\\n\\n        # first, as an example, the normal way would select the \"more\\n        # \\'complete\\' match of choices[1]\"\\n\\n        best = process.extractOne(query, choices)\\n        self.assertEqual(best[0], choices[1])\\n\\n        # now, use the custom scorer\\n\\n        best = process.extractOne(query, choices, scorer=scorer)\\n        self.assertEqual(best[0], choices[0])\\n\\n        best = process.extractOne(query, choices_dict)\\n        self.assertEqual(best[0], choices_dict[1])\\n\\n    def testWithCutoff(self):\\n        choices = [\\n            \"new york mets vs chicago cubs\",\\n            \"chicago cubs at new york mets\",\\n            \"atlanta braves vs pittsbugh pirates\",\\n            \"new york yankees vs boston red sox\"\\n        ]\\n\\n        query = \"los angeles dodgers vs san francisco giants\"\\n\\n        # in this situation, this is an event that does not exist in the list\\n        # we don\\'t want to randomly match to something, so we use a reasonable cutoff\\n\\n        best = process.extractOne(query, choices, score_cutoff=50)\\n        self.assertIsNone(best)\\n\\n        # however if we had no cutoff, something would get returned\\n\\n        # best = process.extractOne(query, choices)\\n        # self.assertIsNotNone(best)\\n\\n    def testWithCutoff2(self):\\n        choices = [\\n            \"new york mets vs chicago cubs\",\\n            \"chicago cubs at new york mets\",\\n            \"atlanta braves vs pittsbugh pirates\",\\n            \"new york yankees vs boston red sox\"\\n        ]\\n\\n        query = \"new york mets vs chicago cubs\"\\n        # Only find 100-score cases\\n        res = process.extractOne(query, choices, score_cutoff=100)\\n        self.assertIsNotNone(res)\\n        best_match, score = res\\n        self.assertIs(best_match, choices[0])\\n\\n    def testEmptyStrings(self):\\n        choices = [\\n            \"\",\\n            \"new york mets vs chicago cubs\",\\n            \"new york yankees vs boston red sox\",\\n            \"\",\\n            \"\"\\n        ]\\n\\n        query = \"new york mets at chicago cubs\"', metadata={'source': 'data/thefuzz/test_thefuzz.py'}),\n",
       " Document(page_content='query = \"new york mets at chicago cubs\"\\n\\n        best = process.extractOne(query, choices)\\n        self.assertEqual(best[0], choices[1])\\n\\n    def testNullStrings(self):\\n        choices = [\\n            None,\\n            \"new york mets vs chicago cubs\",\\n            \"new york yankees vs boston red sox\",\\n            None,\\n            None\\n        ]\\n\\n        query = \"new york mets at chicago cubs\"\\n\\n        best = process.extractOne(query, choices)\\n        self.assertEqual(best[0], choices[1])\\n\\n    def test_list_like_extract(self):\\n        \"\"\"We should be able to use a list-like object for choices.\"\"\"\\n        def generate_choices():\\n            choices = [\\'a\\', \\'Bb\\', \\'CcC\\']\\n            yield from choices\\n        search = \\'aaa\\'\\n        result = [(value, confidence) for value, confidence in\\n                  process.extract(search, generate_choices())]\\n        self.assertGreater(len(result), 0)\\n\\n    def test_dict_like_extract(self):\\n        \"\"\"We should be able to use a dict-like object for choices, not only a\\n        dict, and still get dict-like output.\\n        \"\"\"\\n        try:\\n            from UserDict import UserDict\\n        except ImportError:\\n            from collections import UserDict\\n        choices = UserDict({\\'aa\\': \\'bb\\', \\'a1\\': None})\\n        search = \\'aaa\\'\\n        result = process.extract(search, choices)\\n        self.assertGreater(len(result), 0)\\n        for value, confidence, key in result:\\n            self.assertIn(value, choices.values())\\n\\n    def test_dedupe(self):\\n        \"\"\"We should be able to use a list-like object for contains_dupes\\n        \"\"\"\\n        # Test 1\\n        contains_dupes = [\\'Frodo Baggins\\', \\'Tom Sawyer\\', \\'Bilbo Baggin\\', \\'Samuel L. Jackson\\', \\'F. Baggins\\', \\'Frody Baggins\\', \\'Bilbo Baggins\\']\\n\\n        result = process.dedupe(contains_dupes)\\n        self.assertLess(len(result), len(contains_dupes))\\n\\n        # Test 2\\n        contains_dupes = [\\'Tom\\', \\'Dick\\', \\'Harry\\']\\n\\n        # we should end up with the same list since no duplicates are contained in the list (e.g. original list is returned)\\n        deduped_list = [\\'Tom\\', \\'Dick\\', \\'Harry\\']\\n\\n        result = process.dedupe(contains_dupes)\\n        self.assertEqual(result, deduped_list)\\n\\n    def test_simplematch(self):\\n        basic_string = \\'a, b\\'\\n        match_strings = [\\'a, b\\']\\n\\n        result = process.extractOne(basic_string, match_strings, scorer=fuzz.ratio)\\n        part_result = process.extractOne(basic_string, match_strings, scorer=fuzz.partial_ratio)\\n\\n        self.assertEqual(result, (\\'a, b\\', 100))\\n        self.assertEqual(part_result, (\\'a, b\\', 100))\\n\\n\\nclass TestCodeFormat(unittest.TestCase):\\n    def test_pep8_conformance(self):\\n        pep8style = pycodestyle.StyleGuide(quiet=False)\\n        pep8style.options.ignore = pep8style.options.ignore + tuple([\\'E501\\'])\\n        pep8style.input_dir(\\'thefuzz\\')\\n        result = pep8style.check_files()\\n        self.assertEqual(result.total_errors, 0, \"PEP8 POLICE - WOOOOOWOOOOOOOOOO\")\\n\\nif __name__ == \\'__main__\\':\\n    unittest.main()         # run all tests', metadata={'source': 'data/thefuzz/test_thefuzz.py'})]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "136cae5e",
   "metadata": {},
   "source": [
    "Let's look at an example of a document. It's just code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85a39161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 175 documents\n",
      "\n",
      "------ Start Document ------\n",
      "import unittest\n",
      "import re\n",
      "import pycodestyle\n",
      "\n",
      "from thefuzz import fuzz\n",
      "from thefuzz import process\n",
      "from thefuzz import utils\n",
      "from thefuzz.string_processing import StringProcessor\n",
      "\n",
      "\n",
      "class StringProcessingTest(unittest.TestCase):\n",
      "    def test_replace_non_letters_non_numbers_with_whitespace(self):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(docs)} documents\\n\")\n",
    "print (\"------ Start Document ------\")\n",
    "print (docs[0].page_content[:300])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02634791",
   "metadata": {},
   "source": [
    "Embed and store them in a docstore. This will make an API call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94427072",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2071f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our retriever ready\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0536b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What function do I use if I want to find the most similar item in a list of items?\"\n",
    "output = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9074fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the `extractOne()` function from the `process` module to find the most similar item in a list of items. For example:\n",
      "\n",
      "```\n",
      "from thefuzz import process\n",
      "\n",
      "choices = ['apple', 'banana', 'orange', 'pear']\n",
      "query = 'aple'\n",
      "\n",
      "best_match = process.extractOne(query, choices)\n",
      "print(best_match)\n",
      "```\n",
      "\n",
      "This will output: `('apple', 90)`, which means that the most similar item in the list to the query 'aple' is 'apple', with a similarity score of 90.\n"
     ]
    }
   ],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4aeef60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc\n",
    "query = \"What are the core functions of this code framework thefuzz?\"\n",
    "output = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0d53f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, it appears that thefuzz is a Python module that provides functions for fuzzy string matching. Some of the core functions include `process.extract()`, `process.extractOne()`, and `process.extractBests()`, which are used to extract the best matches for a given string or list of strings. Other functions include `WRatio`, `UWRatio`, and `QRatio`, which are used to calculate different types of string similarity scores. There are also several utility functions for cleaning and processing strings, such as `full_process()` and `asciidammit()`.\n"
     ]
    }
   ],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f53860e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you write the code to use the process.extractOne() function? Only respond with code. No other text or explanation\"\n",
    "output = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27e56a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process.extractOne(query, choices)\n"
     ]
    }
   ],
   "source": [
    "print (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7deae05",
   "metadata": {},
   "source": [
    "[Â¡Shibby!](https://thumbs.gfycat.com/WateryBeneficialDeermouse-size_restricted.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3b2783a",
   "metadata": {},
   "source": [
    "## Case7ï¼šInteracting with APIs æ¥å£è°ƒç”¨ï¼Œåªè¦ç»™æ¥å£æ–‡æ¡£å³å¯~\n",
    "\n",
    "*[LangChain API Interaction Docs](https://python.langchain.com/en/latest/use_cases/apis.html)*\n",
    "\n",
    "If the data or action you need is behind an API, you'll need your LLM to interact with APIs\n",
    "\n",
    "* **Deep Dive** - Coming Soon\n",
    "* **Examples** - TBD\n",
    "* **Use Cases:** Understand a request from a user and carry out an action, be able to automate more real-world workflows\n",
    "\n",
    "This topic is closely related to Agents and Plugins, though we'll look at a simple use case for this section. For more information, check out [LangChain + plugins](https://python.langchain.com/en/latest/use_cases/agents/custom_agent_with_plugin_retrieval_using_plugnplai.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "352685c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b834fe",
   "metadata": {},
   "source": [
    "LangChain's APIChain has the ability to read API documentation and understand which endpoint it needs to call.\n",
    "\n",
    "In this case I wrote (purposefully sloppy) API documentation to demonstrate how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3ff4b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_docs = \"\"\"\n",
    "\n",
    "BASE URL: https://restcountries.com/\n",
    "\n",
    "API Documentation:\n",
    "\n",
    "The API endpoint /v3.1/name/{name} Used to find informatin about a country. All URL parameters are listed below:\n",
    "    - name: Name of country - Ex: italy, france\n",
    "    \n",
    "The API endpoint /v3.1/currency/{currency} Uesd to find information about a region. All URL parameters are listed below:\n",
    "    - currency: 3 letter currency. Example: USD, COP\n",
    "    \n",
    "Woo! This is my documentation\n",
    "\"\"\"\n",
    "\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, api_docs, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "221aa3a6",
   "metadata": {},
   "source": [
    "Let's try to make an API call that is meant for the country endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6d9cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"RÃ©publique franÃ§aise\",\"common\":\"France\"}}},\"tld\":[\".fr\"],\"cca2\":\"FR\",\"ccn3\":\"250\",\"cca3\":\"FRA\",\"cioc\":\"FRA\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"â‚¬\"}},\"idd\":{\"root\":\"+3\",\"suffixes\":[\"3\"]},\"capital\":[\"Paris\"],\"altSpellings\":[\"FR\",\"French Republic\",\"RÃ©publique franÃ§aise\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"translations\":{\"ara\":{\"official\":\"Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©\",\"common\":\"ÙØ±Ù†Ø³Ø§\"},\"bre\":{\"official\":\"Republik FraÃ±s\",\"common\":\"FraÃ±s\"},\"ces\":{\"official\":\"FrancouzskÃ¡ republika\",\"common\":\"Francie\"},\"cym\":{\"official\":\"French Republic\",\"common\":\"France\"},\"deu\":{\"official\":\"FranzÃ¶sische Republik\",\"common\":\"Frankreich\"},\"est\":{\"official\":\"Prantsuse Vabariik\",\"common\":\"Prantsusmaa\"},\"fin\":{\"official\":\"Ranskan tasavalta\",\"common\":\"Ranska\"},\"fra\":{\"official\":\"RÃ©publique franÃ§aise\",\"common\":\"France\"},\"hrv\":{\"official\":\"Francuska Republika\",\"common\":\"Francuska\"},\"hun\":{\"official\":\"Francia KÃ¶ztÃ¡rsasÃ¡g\",\"common\":\"FranciaorszÃ¡g\"},\"ita\":{\"official\":\"Repubblica francese\",\"common\":\"Francia\"},\"jpn\":{\"official\":\"ãƒ•ãƒ©ãƒ³ã‚¹å…±å’Œå›½\",\"common\":\"ãƒ•ãƒ©ãƒ³ã‚¹\"},\"kor\":{\"official\":\"í”„ë‘ìŠ¤ ê³µí™”êµ­\",\"common\":\"í”„ë‘ìŠ¤\"},\"nld\":{\"official\":\"Franse Republiek\",\"common\":\"Frankrijk\"},\"per\":{\"official\":\"Ø¬Ù…Ù‡ÙˆØ±ÛŒ ÙØ±Ø§Ù†Ø³Ù‡\",\"common\":\"ÙØ±Ø§Ù†Ø³Ù‡\"},\"pol\":{\"official\":\"Republika Francuska\",\"common\":\"Francja\"},\"por\":{\"official\":\"RepÃºblica Francesa\",\"common\":\"FranÃ§a\"},\"rus\":{\"official\":\"Ğ¤Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ°Ñ Ğ ĞµÑĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°\",\"common\":\"Ğ¤Ñ€Ğ°Ğ½Ñ†Ğ¸Ñ\"},\"slk\":{\"official\":\"FrancÃºzska republika\",\"common\":\"FrancÃºzsko\"},\"spa\":{\"official\":\"RepÃºblica francÃ©s\",\"common\":\"Francia\"},\"srp\":{\"official\":\"Ğ¤Ñ€Ğ°Ğ½Ñ†ÑƒÑĞºĞ° Ğ ĞµĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°\",\"common\":\"Ğ¤Ñ€Ğ°Ğ½Ñ†ÑƒÑĞºĞ°\"},\"swe\":{\"official\":\"Republiken Frankrike\",\"common\":\"Frankrike\"},\"tur\":{\"official\":\"Fransa Cumhuriyeti\",\"common\":\"Fransa\"},\"urd\":{\"official\":\"Ø¬Ù…ÛÙˆØ±ÛŒÛ ÙØ±Ø§Ù†Ø³\",\"common\":\"ÙØ±Ø§Ù†Ø³\"},\"zho\":{\"official\":\"æ³•å…°è¥¿å…±å’Œå›½\",\"common\":\"æ³•å›½\"}},\"latlng\":[46.0,2.0],\"landlocked\":false,\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"area\":551695.0,\"demonyms\":{\"eng\":{\"f\":\"French\",\"m\":\"French\"},\"fra\":{\"f\":\"FranÃ§aise\",\"m\":\"FranÃ§ais\"}},\"flag\":\"\\uD83C\\uDDEB\\uD83C\\uDDF7\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/g7QxxSFsWyTPKuzd7\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/1403916\"},\"population\":67391582,\"gini\":{\"2018\":32.4},\"fifa\":\"FRA\",\"car\":{\"signs\":[\"F\"],\"side\":\"right\"},\"timezones\":[\"UTC-10:00\",\"UTC-09:30\",\"UTC-09:00\",\"UTC-08:00\",\"UTC-04:00\",\"UTC-03:00\",\"UTC+01:00\",\"UTC+02:00\",\"UTC+03:00\",\"UTC+04:00\",\"UTC+05:00\",\"UTC+10:00\",\"UTC+11:00\",\"UTC+12:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/fr.png\",\"svg\":\"https://flagcdn.com/fr.svg\",\"alt\":\"The flag of France is composed of three equal vertical bands of blue, white and red.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/fr.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/fr.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[48.87,2.33]},\"postalCode\":{\"format\":\"#####\",\"regex\":\"^(\\\\d{5})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France is an officially-assigned, independent country located in Western Europe. Its capital is Paris and its official language is French. Its currency is the Euro (â‚¬). It has a population of 67,391,582 and its borders are with Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain, and Switzerland.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me information about france?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09235fc3",
   "metadata": {},
   "source": [
    "Let's try to make an API call that is meant for the currency endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2735073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/currency/COP\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"Colombia\",\"official\":\"Republic of Colombia\",\"nativeName\":{\"spa\":{\"official\":\"RepÃºblica de Colombia\",\"common\":\"Colombia\"}}},\"tld\":[\".co\"],\"cca2\":\"CO\",\"ccn3\":\"170\",\"cca3\":\"COL\",\"cioc\":\"COL\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"COP\":{\"name\":\"Colombian peso\",\"symbol\":\"$\"}},\"idd\":{\"root\":\"+5\",\"suffixes\":[\"7\"]},\"capital\":[\"BogotÃ¡\"],\"altSpellings\":[\"CO\",\"Republic of Colombia\",\"RepÃºblica de Colombia\"],\"region\":\"Americas\",\"subregion\":\"South America\",\"languages\":{\"spa\":\"Spanish\"},\"translations\":{\"ara\":{\"official\":\"Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© ÙƒÙˆÙ„ÙˆÙ…Ø¨ÙŠØ§\",\"common\":\"ÙƒÙˆÙ„ÙˆÙ…Ø¨ÙŠØ§\"},\"bre\":{\"official\":\"Republik Kolombia\",\"common\":\"Kolombia\"},\"ces\":{\"official\":\"KolumbijskÃ¡ republika\",\"common\":\"Kolumbie\"},\"cym\":{\"official\":\"Gweriniaeth Colombia\",\"common\":\"Colombia\"},\"deu\":{\"official\":\"Republik Kolumbien\",\"common\":\"Kolumbien\"},\"est\":{\"official\":\"Colombia Vabariik\",\"common\":\"Colombia\"},\"fin\":{\"official\":\"Kolumbian tasavalta\",\"common\":\"Kolumbia\"},\"fra\":{\"official\":\"RÃ©publique de Colombie\",\"common\":\"Colombie\"},\"hrv\":{\"official\":\"Republika Kolumbija\",\"common\":\"Kolumbija\"},\"hun\":{\"official\":\"Kolumbiai KÃ¶ztÃ¡rsasÃ¡g\",\"common\":\"Kolumbia\"},\"ita\":{\"official\":\"Repubblica di Colombia\",\"common\":\"Colombia\"},\"jpn\":{\"official\":\"ã‚³ãƒ­ãƒ³ãƒ“ã‚¢å…±å’Œå›½\",\"common\":\"ã‚³ãƒ­ãƒ³ãƒ“ã‚¢\"},\"kor\":{\"official\":\"ì½œë¡¬ë¹„ì•„ ê³µí™”êµ­\",\"common\":\"ì½œë¡¬ë¹„ì•„\"},\"nld\":{\"official\":\"Republiek Colombia\",\"common\":\"Colombia\"},\"per\":{\"official\":\"Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ú©Ù„Ù…Ø¨ÛŒØ§\",\"common\":\"Ú©Ù„Ù…Ø¨ÛŒØ§\"},\"pol\":{\"official\":\"Republika Kolumbii\",\"common\":\"Kolumbia\"},\"por\":{\"official\":\"RepÃºblica da ColÃ´mbia\",\"common\":\"ColÃ´mbia\"},\"rus\":{\"official\":\"Ğ ĞµÑĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ° ĞšĞ¾Ğ»ÑƒĞ¼Ğ±Ğ¸Ñ\",\"common\":\"ĞšĞ¾Ğ»ÑƒĞ¼Ğ±Ğ¸Ñ\"},\"slk\":{\"official\":\"KolumbijskÃ¡ republika\",\"common\":\"Kolumbia\"},\"spa\":{\"official\":\"RepÃºblica de Colombia\",\"common\":\"Colombia\"},\"srp\":{\"official\":\"Ğ ĞµĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ° ĞšĞ¾Ğ»ÑƒĞ¼Ğ±Ğ¸Ñ˜Ğ°\",\"common\":\"ĞšĞ¾Ğ»ÑƒĞ¼Ğ±Ğ¸Ñ˜Ğ°\"},\"swe\":{\"official\":\"Republiken Colombia\",\"common\":\"Colombia\"},\"tur\":{\"official\":\"Kolombiya Cumhuriyeti\",\"common\":\"Kolombiya\"},\"urd\":{\"official\":\"Ø¬Ù…ÛÙˆØ±ÛŒÛ Ú©ÙˆÙ„Ù…Ø¨ÛŒØ§\",\"common\":\"Ú©ÙˆÙ„Ù…Ø¨ÛŒØ§\"},\"zho\":{\"official\":\"å“¥ä¼¦æ¯”äºšå…±å’Œå›½\",\"common\":\"å“¥ä¼¦æ¯”äºš\"}},\"latlng\":[4.0,-72.0],\"landlocked\":false,\"borders\":[\"BRA\",\"ECU\",\"PAN\",\"PER\",\"VEN\"],\"area\":1141748.0,\"demonyms\":{\"eng\":{\"f\":\"Colombian\",\"m\":\"Colombian\"},\"fra\":{\"f\":\"Colombienne\",\"m\":\"Colombien\"}},\"flag\":\"\\uD83C\\uDDE8\\uD83C\\uDDF4\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/RdwTG8e7gPwS62oR6\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/120027\"},\"population\":50882884,\"gini\":{\"2019\":51.3},\"fifa\":\"COL\",\"car\":{\"signs\":[\"CO\"],\"side\":\"right\"},\"timezones\":[\"UTC-05:00\"],\"continents\":[\"South America\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/co.png\",\"svg\":\"https://flagcdn.com/co.svg\",\"alt\":\"The flag of Colombia is composed of three horizontal bands of yellow, blue and red, with the yellow band twice the height of the other two bands.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/co.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/co.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[4.71,-74.07]}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The currency of Colombia is the Colombian peso (COP), symbolized by the \"$\" sign.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me about the currency COP?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d5be7e0",
   "metadata": {},
   "source": [
    "In both cases the APIChain read the instructions and understood which API call it needed to make.\n",
    "\n",
    "Once the response returned, it was parsed and then my question was answered. Awesome ğŸ’"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90e0f275",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "\n",
    "*[LangChain Chatbot Docs](https://python.langchain.com/en/latest/use_cases/chatbots.html)*\n",
    "\n",
    "Chatbots use many of the tools we've already looked at with the addition of an important topic: Memory. There are a ton of different [types of memory](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html), tinker to see which is best for you.\n",
    "\n",
    "* **Deep Dive** - Coming Soon\n",
    "* **Examples** - [ChatBase](https://www.chatbase.co/?via=greg) (Affiliate link), [NexusGPT](https://twitter.com/achammah1/status/1649482899253501958?s=20), [ChatPDF](https://www.chatpdf.com/)\n",
    "* **Use Cases:** Have a real time interaction with a user, provide an approachable UI for users to ask natural language questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7dca0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Chat specific components\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53b86e88",
   "metadata": {},
   "source": [
    "For this use case I'm going to show you how to customize the context that is given to a chatbot.\n",
    "\n",
    "You could pass instructions on how the bot should respond, but also any additional relevant information it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "547aefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a chatbot that is unhelpful.\n",
    "Your goal is to not help the user but only make jokes.\n",
    "Take what the user is saying and make a joke out of it\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "475822a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(openai_api_key=openai_api_key), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a9f557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "Chatbot:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer'))).\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer'))).\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer'))).\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer'))).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Well, if it's a vegetable, it's definitely not a pair!\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ccï¼š\n",
    "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20ae6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Neither, it's a delicious combination of both!\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd87e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI:  Well, if it's a vegetable, it's definitely not a pair!\n",
      "Human: What was one of the fruits I first asked you about?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An apple a day keeps the doctor away!'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"What was one of the fruits I first asked you about?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8db86471",
   "metadata": {},
   "source": [
    "Notice how my 1st interaction was put into the prompt of my 2nd interaction. This is the memory piece at work.\n",
    "\n",
    "There are many ways to structure a conversation, check out the different ways on the [docs](https://python.langchain.com/en/latest/use_cases/chatbots.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144e0d09",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "*[LangChain Agent Docs](https://python.langchain.com/en/latest/modules/agents.html)*\n",
    "\n",
    "Agents are one of the hottest [ğŸ”¥](https://media.tenor.com/IH7C6xNbkuoAAAAC/so-hot-right-now-trending.gif) topics in LLMs. Agents are the decision makers that can look a data, reason about what the next action should be, and execute that action for you via tools\n",
    "\n",
    "* **Deep Dive** - [Introduction to agents](https://youtu.be/2xxziIWmaSA?t=1972), [LangChain Agents Webinar](https://www.crowdcast.io/c/46erbpbz609r), much deeper dive coming soon\n",
    "* **Examples** - TBD\n",
    "* **Use Cases:** Run programs autonomously without the need for human input\n",
    "\n",
    "Examples of advaned uses of agents appear in [BabyAGI](https://github.com/yoheinakajima/babyagi) and [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6d2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Agent imports\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Tool imports\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.utilities import TextRequestsWrapper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e7ab35",
   "metadata": {},
   "source": [
    "For this example I'm going to pull google search results. You may want to do this if you need a list of websites for a research project.\n",
    "\n",
    "You can sign up for both of these keys at the urls below\n",
    "\n",
    "[GOOGLE_API_KEY](https://console.cloud.google.com/apis/credentials)\n",
    "[GOOGLE_CSE_ID](https://programmablesearchengine.google.com/controlpanel/create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc401c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apikey import GOOGLE_API_KEY, GOOGLE_CSE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef374dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3235ccc",
   "metadata": {},
   "source": [
    "Initialize both the tools you'll be using. For this example we'll search google and also give the LLM the ability to execute python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55903997",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)\n",
    "\n",
    "requests = TextRequestsWrapper()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7859aed9",
   "metadata": {},
   "source": [
    "Put both your tools in a toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e60591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Requests\",\n",
    "        func=requests.get,\n",
    "        description=\"Useful for when you to make a request to a URL\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21f7c19e",
   "metadata": {},
   "source": [
    "Create your agent by giving it the tools, LLM and the type of agent that it should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4ad2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da11d1dc",
   "metadata": {},
   "source": [
    "Now ask it a question, I'm going to give it one that it should go to Google for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b027ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent({\"input\":\"What is the capital of canada?\"})\n",
    "response['output']\n",
    "\n",
    "# ccï¼šä¸çŸ¥ä¸ºå•¥ä¼štime out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db969cf",
   "metadata": {},
   "source": [
    "Great, that's correct. Now let's ask a question that requires listing the currect directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e516015",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent({\"input\":\"Tell me what the comments are about on this webpage https://news.ycombinator.com/item?id=34425779\"})\n",
    "response['output']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad644451",
   "metadata": {},
   "source": [
    "## FIN\n",
    "\n",
    "Wow! You made it all the way down to the bottom.\n",
    "\n",
    "Where do you go from here?\n",
    "\n",
    "The world of AI is massive and use cases will continue to grow. I'm personally most excited about the idea of use cases we don't know about yet.\n",
    "\n",
    "What else should we add to this list?\n",
    "\n",
    "Check out this [repo's ReadMe](https://github.com/gkamradt/langchain-tutorials) for more inspiration\n",
    "Check out more tutorials on [YouTube](https://www.youtube.com/@DataIndependent)\n",
    "\n",
    "I'd love to see what projects you build. Tag me on [Twitter](https://twitter.com/GregKamradt)!\n",
    "\n",
    "Have something you'd like to edit? See our [contribution guide](https://github.com/gkamradt/langchain-tutorials) and throw up a PR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
